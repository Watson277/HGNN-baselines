from torch_geometric.nn.models import MetaPath2Vec
import torch
import torch.nn.functional as F

# åŠ è½½ HeteroData å¯¹è±¡
data = torch.load('./datasets/Yelp JSON/yelp.pt')
print(data)

target_node_type = 'business'

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
data = data.to(device)


metapaths = [
    ('user', 'friends', 'user'), 
    ('user', 'writes', 'review'), 
    ('review', 'about', 'business')
]

model = MetaPath2Vec(
    edge_index_dict=data.edge_index_dict,
    embedding_dim=64,
    metapath=metapaths,  # ğŸ‘ˆ ä¼ å…¥ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ï¼ˆä¸æ˜¯åˆ—è¡¨çš„åˆ—è¡¨ï¼‰
    walk_length=3,
    context_size=3,
    walks_per_node=5,
    num_negative_samples=5,
    num_nodes_dict={key: data[key].num_nodes for key in data.node_types},
    sparse=True
)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
data = data.to(device)

# è·å–æ¨¡å‹å†…ç½®çš„éšæœºæ¸¸èµ°é‡‡æ ·å™¨
loader = model.loader(batch_size=128, shuffle=True, num_workers=0)

# ä¼˜åŒ–å™¨ï¼ˆåªä¼˜åŒ–åµŒå…¥å‚æ•°ï¼‰
optimizer = torch.optim.SparseAdam(model.parameters(), lr=0.01)

# è®­ç»ƒå‡½æ•°
def train():
    model.train()
    total_loss = 0
    for i, (pos_rw, neg_rw) in enumerate(loader):
        pos_rw, neg_rw = pos_rw.to(device), neg_rw.to(device)
        optimizer.zero_grad()
        loss = model.loss(pos_rw, neg_rw)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / (i + 1)

# åµŒå…¥å‡½æ•°ï¼ˆåªé’ˆå¯¹æŸä¸€ç±»èŠ‚ç‚¹ï¼‰
@torch.no_grad()
def test():
    model.eval()
    z = model(target_node_type)  # å–å‡º author çš„åµŒå…¥
    y = data[target_node_type].y
    split = data[target_node_type].train_mask, data[target_node_type].test_mask

    accs = []
    for mask in split:
        clf = torch.nn.Linear(z.size(1), y.max().item() + 1).to(device)
        optimizer = torch.optim.Adam(clf.parameters(), lr=0.01, weight_decay=5e-4)

        best_acc = 0
        for _ in range(50):  # ç”¨ Logistic Regression åˆ†ç±»
            clf.train()
            optimizer.zero_grad()
            loss = F.cross_entropy(clf(z[mask], ), y[mask])


            clf.eval()
            pred = clf(z[mask]).argmax(dim=1)
            acc = (pred == y[mask]).float().mean().item()
            best_acc = max(best_acc, acc)
        accs.append(best_acc)
    return accs

# è®­ç»ƒä¸»å¾ªç¯
for epoch in range(1, 51):
    loss = train()
    train_acc, test_acc = test()
    print(f"Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}")


train_acc, test_acc = test()
print(f"Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}")